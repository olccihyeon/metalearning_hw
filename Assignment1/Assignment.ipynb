{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bd568d-3568-4a08-914a-1d671e6055da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment1: Prototypical Networks\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/oscarknagg/few-shot/master/assets/proto_nets_diagram.png\"/></center>\n",
    "\n",
    "> Figure 1: Prototypical networks in a nutshell. In a 3-way 5-shot classification task, the class\n",
    "prototypes c1, c2, c3 are computed from each class’s support features (colored circles). The\n",
    "prototypes define decision boundaries based on Euclidean distance. A query example $x$\n",
    "is determined to be class 2 since its features (white circle) lie within that class’s decision\n",
    "region.\n",
    "\n",
    "\n",
    "In this assignment, let's use [**prototypical networks**](https://papers.nips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) to solve the *n-way k-shot few-shot learning* problem with the [Omniglot](https://www.tensorflow.org/datasets/catalog/omniglot) dataset. \n",
    "\n",
    "- **IMPORTANT**: 과제 제출하실 때 레포트를 pdf로 변환하고, `Assignment.ipynb`, `dataloader.py`, `model.py`, `train.py`, 모두 압축해서 **이름_학번_Assignment1.zip** 형식으로 ETL에 올리시길 바랍니다.\n",
    "- Assignment 1 is due 23:59 PM 4/14. Note that 10%p of your score will be deducted for every hour you are late.\n",
    "- Make sure you are using the latest version of [TensorFlow](https://www.tensorflow.org/api_docs/python/tf), or at least the 2.3 version.\n",
    "- *If you have any questions regarding the assignment, please contact me at steve2972@snu.ac.kr*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd832e9-f3f6-475f-9349-3caef2d1c554",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "We define a **task** to be an *n-way k-shot* image classification problem. The maximum number of tasks that can be created from a dataset can be defined as $M \\choose N$$\\cdot$$ K \\choose k$ where $M$=the number of classes in the dataset, $N$=the number of classes in a task, $K$=the total number of images in a class, and $k$=the number of shots in a class. In this problem, we define a method to create an upper bound to the number of tasks used during training. \n",
    "\n",
    "**Problem**: In the `dataloader.py` file, complete the implementation of the `DataLoader.generate_task_list` method and the `DataLoader.data_generator` method. Details concerning the implementation are as follows:\n",
    "\n",
    "- The `generate_task_list` method generates a **list of dictionaries** where each dictionary comprises a **task** in the form of `{class_name: [random sequence of image indexes]}`\n",
    "    - For example, a 2-way 5-shot task for both the support & query datasets can be generated as `{'A': [1,3,5,7,9,2,4,6,8, 10], 'B': [8,6,4,10,2,1,5,3,7,9]}`\n",
    "    - Note that the length of the random sequence of image indexes should equal the # of shots in the support dataset $+$ # of shots in the query dataset.\n",
    "    \n",
    "- The `data_generator` method generates a **support** and a **query dataset** where each dataset is in the form of a numpy array with shape `[num_way, num_shot, image_width, image_height, num_channels]`.\n",
    "    - Note that `data_generator` is created based on tasks which have the form `{class_name: [random sequence of image indexes]}`\n",
    "    \n",
    "If the `dataloader.py` file is implemented correctly, the following code should run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2382c2-8f38-4179-a7bb-1387c551bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train Omniglot dataset\n",
      "Finished preprocessing\n",
      "Preprocessing test Omniglot dataset\n",
      "Finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "\n",
    "num_ways = 5\n",
    "support_shots = 5\n",
    "query_shots = 5\n",
    "\n",
    "train_dataset = DataLoader('train', n_way=num_ways, n_support=support_shots, n_query=query_shots)\n",
    "val_dataset = DataLoader('test', n_way=num_ways, n_support=support_shots, n_query=query_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d02dfadb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.8.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)\n",
    "tf.test.is_gpu_available()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d72f0022",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 15671342647079008715\n",
       " xla_global_id: -1]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bff77-39a0-4950-8305-a964d802b361",
   "metadata": {},
   "source": [
    "**If implemented correctly, we can also visualize a random task using the `visualize_random_task` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3e14ae5c-8949-4afb-90d7-99505264df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAA+CAYAAABtAQ2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAEoklEQVR4nO3dQU7yQBjG8aefLHWlOzGQeA1NPISulYUnUG+BJ9BEWest9BgmSnCpiYm4UsO3IFMLglTbad9O/7+Nik07w5TpO9O3QzQajQQAVvwruwAAkESnBMAUOiUAptApATCFTgmAKXRKAExpLPh/CPkCUYptqGd1LKpnHeooBVxPIiUAptApATCFTgmAKV47pShKMzRGVdCeKAKREgBT6JSAGur1ehoOhxoOh2UX5Rs6JQCmRAuWLsmUCxFFkQwsjeI95yOKIj08PEiSWq1Wll1lKkaKbXKrp1NCfclTGgv2nF2UPAlMaLfbE38vLy9Lkl5fX0soDULE8A2AKd6Hb2l4HuIVEgrv7u5K+oocZrm4uMhymIXFSLFNbsPx29tbSdL29vbXzosZqnsZvvV6PUnSwcFBbaYc3Dl7dXWVZVeZijHrRSIlAKZ4j5TmTaZ1Op24h357exsfzM8VqpCrzm9UuZ7zyp58D0qOfHOLlNxr+/v7f9llFqWcs5eXl5IKrS+REgD7CptT+uk4KysrkhQncuV8pTVze9W9H09PT5Kk1dXVLIf9tvsU23hN8Wg0xjdzPz8/xwcrJyLMHCmlEcI8qDtnd3Z2JGki3ePj40OStLS0lOUwC4sx60UTKQHudrL70DYajfhNCZG7rR7abXTXZq4dO52OJO8T/Lmbd4Hp9/tx200PfwxMjv/Z/f39xN9RFMUXGHfhdBfSIjB8A2CK10ip2+3q+Pg49fbuahNFkfr9vqRSs01zd3NzI2nyNjrsmXfOtVqtbxHR4eGhJDO32HORrKOLCN3PIiJCIiUApniNlI6Ojn4VKSW5sXtVxuppyru1tVVQaVCUs7MzSdLLy4uur69LLk3+kqMX99P3Z5JICYApJjul6SfRAeuS80i9Xi9OMQjFaDQqbNRiIiVgWkiT26iP9fV1SV+5TiVkghfi8fFRktRsNr3s32SkBKC+TEZKqKaTk5Oyi/BnLqpJm9E9i4sgQv+CBZex7wuREgBTTEZKp6enZRcBC7j1lNyE7vn5efw/l/pQtcdLnBATd/P005pheSBSAmCKiUjp+flZkrS2tha/5mtm3zd3lXWm17SuunnzJe12O36Mpqpt5/w1cdfNKYUo+bB8zqtbfOO1U0o20m8m/6qSxT1Lmk6oyCeufel2u5LGWfsYt/v0BSkE05/b9/d378dk+AbAFK+RUjKMv7u7kyRtbm76PGTpqhzlpeHWgPI92WmdWyvKLSGb/D2EpMnp9i3yvCZSAmCK1+VwpWLXYZlXhBTbZCrc3t5e/IR4yPWsyDceF/olEC7ZMuf0h0KXwx0MBpJmr/NVxrK/REoATCFSGstUuOTazSHXs26RUuhtOc2dw81mM07v8MzuFwcAKJ7reKwtPsjwDYApREo5mLWgPKqrLosMbmxslF2EmYiUAJhCpISFQlvatc6q0JZESgBMISVgLIQJIeo5Voe0BymHerqkyRJXdSgnJcBAAwOp1eV8HQwGZpeYYfgGwBTvwzcDGNZ8qUM961BHKeB6EikBMGVRpAQAhSJSAmAKnRIAU+iUAJhCpwTAFDolAKbQKQEw5T/Rbri26en+MAAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAA+CAYAAABtAQ2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAEZElEQVR4nO3dT1LiQBTH8V+mXMaVLq1SSw/gATyGrA0Lb4CnUE7gAlyLp9BrUELpUleytTIL6kUmw58E0vAC308VNaNmSDIdXl53XrdRmqYCAC/+bPoAAGASQQmAKwQlAK4QlAC4QlAC4ApBCYArewt+vg31AlGBbTjP+lh0nrtwjtIWnyeZEgBXCEoAXCEoAXCFoATAFYISAFcISgBcISgBcIWgBBQURdHM18fHx6YPb2tEC9ZT2toCrRzOsz7WXjwZRf/v8vv7W5K0v7//u+Pq1ibb6bYkUwLgSvCgNC/ltRfgUf76TNM0e8VxrDiOs6+nbV9XjUZjo59VMiUAriyakLuyef1si7w3NzeSpIeHh9CHszb9fl+SdH5+/t/PXl5eJEmXl5drPSYUY9eltY+11zxpmmb/ztr+7Ows0BFWa1oGlCTJP193u91s29Dr+pMpAXDFxdM3i9SBIvBan2QMh0NJ0snJSfbn29ubJOn29laSdH9/L0l6f3/X0dFRVbve6Sc2EzZ2jnt7447Hz8/P+ECWv57X2panp6eSpKurK0nS3d3dzG3zY2wrmnqeroKSpYydTqfSty+wTXXPcnOp8LT/33a7LUlqtVo8Ri7PbVCyG5HdmOoSlMoqco0Xfatp36T7BsAVF5lSs9mU9DuYVnE3zu1dJ4qiLGV+enpa+e0KbEOmFNCuZEqmgmEXMiUA/gUNSlEUaTQaaTQahdxNbSVJol6vp16vp36/nz1KrqN5xbGHh4ebPjzMkW8veyCzKWRKAFwJUjz5+PiY/T2O4xC7cKvMk8ROp5ONo72+vkqqT8GdsSeJ0uyxhckpCoPBQJJ0fHwc/NhWVXbMxMaS6sLGwEzoosiiggQl+0DmT3qbWY2KsVqkbddqtSSNa65mSdM0q8eya8JuVhcXF4Uqptcp35Zl2Y0Gy6H7BsCV4HPf5rEBNbuz5Ofb1EWj0aiiirfWFlWm5xdBs66RdVu9GA6HS7XlZNft+vq68uPaJWRKAFwJUjxZdt2VQNNLTPBCtGXnA7Xb7WxMJtQ8opygaZw94CiSKdi2SZKUPfegxZPLtuU65oTlrLyTVYs9KZ4EsBOCjCnt2rjKYDAo9aRxckJuhasEbNRkplAkU/r8/Ax5OCuxsoUiipREbJvQK1C6mPsW2FpSYWuog4MDSf9+6PJpstmmpUvKdl9s+8mlXYruasHPV+6+Famlsl8YYLMV6tyWRWb9Pz8/S/pd3mTWdmV3Pe2bdN8AuEKmNBbsrjPJSh8CPTJ2kynNK+3IFxYucbfd2ED3tLbdhoUJs50W6JZ1u90qr18yJQD+kSmNcZ4r+vr6KrQaQJnF+GcImik1m82Z00TiOM5+CWVgO33NkikBcIVMaYzzrA+3K09WaKfbkkwJgCsEJQCuEJQAuEJQAuAKQQmAKwQlAK4QlAC4QlAC4ApBCYArBCUArhCUALiyaO4bAKwVmRIAVwhKAFwhKAFwhaAEwBWCEgBXCEoAXPkLrdib1CTNNmsAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAA+CAYAAABtAQ2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAADVElEQVR4nO3dQVLqQBSF4ZNX7gCHUmCxDVyFE2bAHqDKPcgedAGyCt2GBRkwZAVQxRvwGiNPDYZ096XzfyNLUnTakOPtpDtku91OAGDFn9g7AABFhBIAUwglAKYQSgBMIZQAmEIoATDlquT1FOYLZCdsQz8vR1k/m9BHKeF+UikBMIVQAmAKoQTAFEIJgCmEEgBTyu6+AZ90u11JUp7nkiQWdKNuVEoATKFSqslxBVEVlQeaLis5CVI4Q4JORHOh5ELqt84IpSD9zLLPzUQIUSZP7iXbT4ZvAEyhUtrz2s9A1UWQfv40TB2NRpKkp6cnSdLb25skqd/vn9tskddKKcsyC0Nob8dyPB4ffnbHKSIqJQD2ea2UjiuEMpdcQXzbeOFv4Pk/cNB+un71ej1J0vv7+7fbTiYTPT4+1tZ0yeu1fWYjVkzejuVX5+Tz87MkaTgcVnnLc1ApAbAv+DUll9TFu1OLxaLuZj41ecI2Qa4ppVgppXb37dTq3nN1EeVO6qmWy6UkqdPpnL0LX/4yVigF/DATSh8IpSoN/uurOxmlWk7IH5s8YZva+rleryV9BO1kMtHr66skqd1u/7d9jX1n+AbAPmZ0e3buDG/Y4bk6iqbVakmSrq+vD7+reRrHr1ApATAlWqV0e3vr+wK3CS8vL7F3ARW5yZ8Ii0oJgCnBKyV3hd8tSUjdw8ND7F1ARXd3d7F3oZG40O3Zdru1sJbKm+l0Kkl1ztg2J+XjZxHDNwCmEEoB5Hme7NSA2Wym2WwWezeQEEIJgCnBrym5VeW4bO46S9X1U8B3qJQAmBK8Uoo5fT0W90QE7uJcJve0RgNPamwEKiXPiivLu91u5S8UAJqCUAJgCpMnPet0OofV5SlNCyg+gB6oE5USAFOolAJw15VSun3etDWMTWDlWFIpATAl+DO65/O5JGkwGGiz2dT99l+J/ozum5sbSdJqtdo3lsBXSaX8jO4L+GpyKeEvig0+fLu/v5ekUIEUXZ7nhzBijZh9xZsRzCuLg+EbAFOCD98iaHQpfKQJ/WxCH6WE+0mlBMAUQgmAKYQSAFMIJQCmEEoATCGUAJhCKAEwhVACYErZ5EkACIpKCYAphBIAUwglAKYQSgBMIZQAmEIoATDlL9noATFO52u7AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset.visualize_random_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3d864-c566-4f3f-b607-f10668d246df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Next we will implement the prototypical network and examine how the performance of the model changes when **the total number of tasks is changed from 1k to 10k**.\n",
    "\n",
    "As discussed in the lecture, the basic idea of protonets is to learn a mapping $f_\\theta(\\cdot)$ from images to features such that images of the same class are close to each other in a feature space. Central to this is the notion of a *prototype*\n",
    "\n",
    "$$c_n = \\frac{1}{K} \\sum_{(x,y)\\in\\mathcal{D}^{tr}_i:y=n} f_\\theta(x)$$\n",
    "\n",
    "i.e. for task $i$, the prototype of the $n$-th class $c_n$ is defined as the mean of the $K$ feature\n",
    "vectors of that class’s support images. To classify some image $x$, we compute a measure of\n",
    "distance $d$ between $f_\\theta(x)$ and each of the prototypes. We will use the squared Euclidean\n",
    "distance:\n",
    "\n",
    "$$d(f_\\theta(x),c_n) = ||f_\\theta(x) - c_n ||^2_2$$\n",
    "\n",
    "We interpret the negative squared distances as **logits**, or *unnormalized log-probabilities,\n",
    "of* $x$ *belonging to each class*. To obtain the proper probabilities, we apply the softmax\n",
    "operation:\n",
    "\n",
    "$$p_\\theta(y = n|x) = \\frac{\\exp(-d(f_\\theta(x),c_n))}{\\sum^N_{n'=1}\\exp(-d(f_\\theta(x),c_{n'}))}$$\n",
    "\n",
    "Because the softmax operation preserves ordering, the class whose prototype is closest to\n",
    "$f_\\theta(x)$ is naturally interpreted as the most likely class for $x$. To train the model to generalize,\n",
    "we compute prototypes using support data, but minimize the negative log likelihood of the query data\n",
    "\n",
    "$$\\mathcal{J}(\\theta) = \\mathbb{E}_{\\mathcal{T}\\sim p(\\mathcal{T}), (\\mathcal{D}^{tr}, \\mathcal{D}^{ts})\\sim \\mathcal{T}} \\left[ \\frac{1}{NQ} \\sum_{(x^{ts},y^{ts})\\sim\\mathcal{D}^{ts}} -\\log p_\\theta(y=y^{ts} | x^{ts})\\right]$$\n",
    "\n",
    "Notice that this is equivalent to using a cross-entropy loss.\n",
    "\n",
    "We optimize $\\theta$ using Adam, and as is standard for stochastic gradient methods, we approximate the objective for $\\mathcal{J}$ with\n",
    "Monte Carlo estimation on minibatches of tasks. Thus, for one minibatch of size $B$, we have\n",
    "\n",
    "$$\\mathcal{J}(\\theta) \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ \\frac{1}{NQ} \\sum_{(x^{ts},y^{ts})\\sim\\mathcal{D}^{ts}} -\\log p_\\theta(y=y^{ts} | x^{ts})\\right]$$\n",
    "\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "1. In the `model.py` file, complete the implementation of the `Prototypical_Network.call` method. Pay attention to the inline comments.\n",
    "2. Test the model with different amounts of training tasks (e.g. from 1k to 10k).\n",
    "    1.  Create a plot of the validation accuracy using `matplotlib` and report your findings. \n",
    "        - Train and validate the model on 5-way 5-shot tasks for both the support and query datasets.\n",
    "        - [Hint]: you should obtain a query accuracy on the validation split of **at least** 97% with 10k tasks.\n",
    "    2. Also plot how the degree of overfitting (the difference between training accuracy and validation accuracy) changes with different amounts of tasks.\n",
    "3. Repeat (2) with different tasks [5way-1shot, 20way-5shot, 20way-1shot] and report your findings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303b895e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf817cd-de2e-463c-9016-64cee359d2b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "\t-Train Loss:0.636 | Train Acc:81.28%\n",
      "\t-Val Loss:0.764 | Val Acc:74.48%\n",
      "Epoch2\n",
      "\t-Train Loss:0.327 | Train Acc:90.92%\n",
      "\t-Val Loss:0.465 | Val Acc:84.56%\n",
      "Epoch3\n",
      "\t-Train Loss:0.268 | Train Acc:91.16%\n",
      "\t-Val Loss:0.311 | Val Acc:89.16%\n",
      "Epoch4\n",
      "\t-Train Loss:0.172 | Train Acc:94.40%\n",
      "\t-Val Loss:0.252 | Val Acc:91.84%\n",
      "Epoch5\n",
      "\t-Train Loss:0.141 | Train Acc:95.44%\n",
      "\t-Val Loss:0.255 | Val Acc:92.00%\n",
      "Epoch6\n",
      "\t-Train Loss:0.098 | Train Acc:97.24%\n",
      "\t-Val Loss:0.160 | Val Acc:95.08%\n",
      "Epoch7\n",
      "\t-Train Loss:0.090 | Train Acc:97.68%\n",
      "\t-Val Loss:0.174 | Val Acc:94.36%\n",
      "Epoch8\n",
      "\t-Train Loss:0.070 | Train Acc:97.64%\n",
      "\t-Val Loss:0.145 | Val Acc:95.52%\n",
      "Epoch9\n",
      "\t-Train Loss:0.079 | Train Acc:97.04%\n",
      "\t-Val Loss:0.161 | Val Acc:95.32%\n",
      "Epoch10\n",
      "\t-Train Loss:0.081 | Train Acc:97.24%\n",
      "\t-Val Loss:0.169 | Val Acc:94.36%\n",
      "Epoch11\n",
      "\t-Train Loss:0.090 | Train Acc:97.28%\n",
      "\t-Val Loss:0.153 | Val Acc:95.16%\n",
      "Epoch12\n",
      "\t-Train Loss:0.069 | Train Acc:97.60%\n",
      "\t-Val Loss:0.141 | Val Acc:94.92%\n",
      "Epoch13\n",
      "\t-Train Loss:0.056 | Train Acc:98.44%\n",
      "\t-Val Loss:0.142 | Val Acc:95.88%\n",
      "Epoch14\n",
      "\t-Train Loss:0.073 | Train Acc:97.76%\n",
      "\t-Val Loss:0.174 | Val Acc:93.96%\n",
      "Epoch15\n",
      "\t-Train Loss:0.064 | Train Acc:98.36%\n",
      "\t-Val Loss:0.136 | Val Acc:96.04%\n",
      "Epoch16\n",
      "\t-Train Loss:0.045 | Train Acc:98.68%\n",
      "\t-Val Loss:0.101 | Val Acc:96.64%\n",
      "Epoch17\n",
      "\t-Train Loss:0.041 | Train Acc:98.64%\n",
      "\t-Val Loss:0.123 | Val Acc:95.76%\n",
      "Epoch18\n",
      "\t-Train Loss:0.036 | Train Acc:98.72%\n",
      "\t-Val Loss:0.147 | Val Acc:95.68%\n",
      "Epoch19\n",
      "\t-Train Loss:0.054 | Train Acc:98.32%\n",
      "\t-Val Loss:0.131 | Val Acc:95.80%\n",
      "Epoch20\n",
      "\t-Train Loss:0.034 | Train Acc:98.68%\n",
      "\t-Val Loss:0.119 | Val Acc:96.68%\n",
      "Epoch21\n",
      "\t-Train Loss:0.044 | Train Acc:98.12%\n",
      "\t-Val Loss:0.169 | Val Acc:94.88%\n",
      "Epoch22\n",
      "\t-Train Loss:0.038 | Train Acc:98.64%\n",
      "\t-Val Loss:0.134 | Val Acc:96.12%\n",
      "Epoch23\n",
      "\t-Train Loss:0.045 | Train Acc:98.44%\n",
      "\t-Val Loss:0.119 | Val Acc:96.36%\n",
      "Epoch24\n",
      "\t-Train Loss:0.046 | Train Acc:98.32%\n",
      "\t-Val Loss:0.147 | Val Acc:95.68%\n",
      "Epoch25\n",
      "\t-Train Loss:0.044 | Train Acc:98.76%\n",
      "\t-Val Loss:0.128 | Val Acc:95.80%\n",
      "Epoch26\n",
      "\t-Train Loss:0.035 | Train Acc:98.68%\n",
      "\t-Val Loss:0.175 | Val Acc:95.80%\n",
      "Epoch27\n",
      "\t-Train Loss:0.042 | Train Acc:98.68%\n",
      "\t-Val Loss:0.142 | Val Acc:96.12%\n",
      "Epoch28\n",
      "\t-Train Loss:0.034 | Train Acc:98.56%\n",
      "\t-Val Loss:0.139 | Val Acc:96.20%\n",
      "Epoch29\n",
      "\t-Train Loss:0.031 | Train Acc:98.92%\n",
      "\t-Val Loss:0.133 | Val Acc:95.64%\n",
      "Epoch30\n",
      "\t-Train Loss:0.030 | Train Acc:99.00%\n",
      "\t-Val Loss:0.113 | Val Acc:95.96%\n",
      "Epoch31\n",
      "\t-Train Loss:0.031 | Train Acc:98.88%\n",
      "\t-Val Loss:0.148 | Val Acc:95.92%\n",
      "Epoch32\n",
      "\t-Train Loss:0.047 | Train Acc:98.44%\n",
      "\t-Val Loss:0.118 | Val Acc:96.72%\n",
      "Epoch33\n",
      "\t-Train Loss:0.037 | Train Acc:98.84%\n",
      "\t-Val Loss:0.110 | Val Acc:96.24%\n",
      "Epoch34\n",
      "\t-Train Loss:0.039 | Train Acc:98.56%\n",
      "\t-Val Loss:0.115 | Val Acc:96.20%\n",
      "Epoch35\n",
      "\t-Train Loss:0.045 | Train Acc:98.36%\n",
      "\t-Val Loss:0.111 | Val Acc:96.08%\n",
      "Epoch36\n",
      "\t-Train Loss:0.027 | Train Acc:99.24%\n",
      "\t-Val Loss:0.085 | Val Acc:97.48%\n",
      "Epoch37\n",
      "\t-Train Loss:0.035 | Train Acc:98.56%\n",
      "\t-Val Loss:0.096 | Val Acc:97.16%\n",
      "Epoch38\n",
      "\t-Train Loss:0.017 | Train Acc:99.48%\n",
      "\t-Val Loss:0.104 | Val Acc:96.68%\n",
      "Epoch39\n",
      "\t-Train Loss:0.017 | Train Acc:99.44%\n",
      "\t-Val Loss:0.122 | Val Acc:96.60%\n",
      "Epoch40\n",
      "\t-Train Loss:0.022 | Train Acc:99.24%\n",
      "\t-Val Loss:0.131 | Val Acc:96.60%\n",
      "Epoch41\n",
      "\t-Train Loss:0.031 | Train Acc:98.68%\n",
      "\t-Val Loss:0.091 | Val Acc:97.40%\n",
      "Epoch42\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4ba283dcbdbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m metrics = train.train_model(train_dataset, val_dataset, \n\u001b[1;32m      9\u001b[0m                             \u001b[0mn_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                             n_tpe=num_tasks_per_epoch)\n\u001b[0m",
      "\u001b[0;32m~/metalearning_hw/Assignment1/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataset, val_dataset, n_tasks, n_epochs, n_tpe, is_random)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mval_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mon_end_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    945\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    946\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 947\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    948\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2955\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m   2956\u001b[0m     return graph_function._call_flat(\n\u001b[0;32m-> 2957\u001b[0;31m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m   2958\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2959\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1852\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1853\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1854\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1855\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1856\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    502\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    503\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 504\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    506\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 55\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     56\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import train\n",
    "\n",
    "num_tasks = 10000 # The total number of tasks in the predefined task distribution\n",
    "num_epochs = 100 # The number of epochs to train the model\n",
    "num_tasks_per_epoch = 100 # The number of tasks to be trained on for each epoch\n",
    "\n",
    "# metrics = [train accuracies, train losses, validation accuracies, validation losses]\n",
    "metrics = train.train_model(train_dataset, val_dataset, \n",
    "                            n_tasks=num_tasks, n_epochs=num_epochs, \n",
    "                            n_tpe=num_tasks_per_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a6395-a092-440c-92be-d0bf79a0db74",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Finally, we will evaluate the generalization performance of the model by using a **different test setting from a training setting**.\n",
    "\n",
    "<br>\n",
    "\n",
    "As prototypical networks are based on a nonparametric method, we can flexibly train the model with different types of tasks (e.g. 5way1shot tasks + 5way5shot tasks). Let's assume we are allowed to use up to 30 training samples to train a single task. Evaluate the performance of the model when we train the model using **randomly defined training tasks**.\n",
    "\n",
    "\n",
    "\n",
    "**Problem**:\n",
    "\n",
    "1. In the `dataloader.py` file, implement the `random_data_generator` method.\n",
    "2. Using the experimental details outlined below, run experiments and compare with the results from Problem 2\n",
    "    - Note that you do not need to specify a set number of tasks for this problem\n",
    "\n",
    "\n",
    "**Experimental Details**:\n",
    "- *Train setting*: Assume to use exactly 30 images for each task (for support + query). First randomly select $N$ from the set $\\{2, 3, 5, 6, 10, 15\\}$. This means that each $N$ can use $\\{15, 10, 6, 5, 3, 2\\}$ samples per class respectively. By properly splitting the number of samples into 2 splits (support/query), we can randomly define different tasks.\n",
    "    - For example, an example support/query task can be defined as support={5-way 4-shot}, and query={5-way 2-shot}.\n",
    "    - *Extra Credit (optional)*: implement and test a method where we use a flexible number of images (not limited to just 30) where $N$ can also be defined randomly.\n",
    "- *Test setting*: 5way 5 shot (5$\\cdot$5 for support + 5$\\cdot$1 for query = 30 images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b81f55-0b23-41b0-9a2c-44164eb58f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ways = 5\n",
    "support_shots = 5\n",
    "query_shots = 1\n",
    "\n",
    "train_dataset = DataLoader('train')\n",
    "val_dataset = DataLoader('test', n_way=num_ways, n_support=support_shots, n_query=query_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d925446-05fd-4ea0-9422-aab4c82e0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # The number of epochs to train the model\n",
    "num_tasks_per_epoch = 100 # The number of tasks to be trained on for each epoch\n",
    "\n",
    "# metrics = [train accuracies, train losses, validation accuracies, validation losses]\n",
    "metrics = train.train_model(train_dataset, val_dataset, \n",
    "                            n_tasks=num_tasks, n_epochs=num_epochs, \n",
    "                            n_tpe=num_tasks_per_epoch, is_random=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
