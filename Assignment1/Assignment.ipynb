{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "74bd568d-3568-4a08-914a-1d671e6055da",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Assignment1: Prototypical Networks\n",
    "\n",
    "\n",
    "<center><img src=\"https://raw.githubusercontent.com/oscarknagg/few-shot/master/assets/proto_nets_diagram.png\"/></center>\n",
    "\n",
    "> Figure 1: Prototypical networks in a nutshell. In a 3-way 5-shot classification task, the class\n",
    "prototypes c1, c2, c3 are computed from each class’s support features (colored circles). The\n",
    "prototypes define decision boundaries based on Euclidean distance. A query example $x$\n",
    "is determined to be class 2 since its features (white circle) lie within that class’s decision\n",
    "region.\n",
    "\n",
    "\n",
    "In this assignment, let's use [**prototypical networks**](https://papers.nips.cc/paper/2017/hash/cb8da6767461f2812ae4290eac7cbc42-Abstract.html) to solve the *n-way k-shot few-shot learning* problem with the [Omniglot](https://www.tensorflow.org/datasets/catalog/omniglot) dataset. \n",
    "\n",
    "- **IMPORTANT**: 과제 제출하실 때 레포트를 pdf로 변환하고, `Assignment.ipynb`, `dataloader.py`, `model.py`, `train.py`, 모두 압축해서 **이름_학번_Assignment1.zip** 형식으로 ETL에 올리시길 바랍니다.\n",
    "- Assignment 1 is due 23:59 PM 4/14. Note that 10%p of your score will be deducted for every hour you are late.\n",
    "- Make sure you are using the latest version of [TensorFlow](https://www.tensorflow.org/api_docs/python/tf), or at least the 2.3 version.\n",
    "- *If you have any questions regarding the assignment, please contact me at steve2972@snu.ac.kr*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cd832e9-f3f6-475f-9349-3caef2d1c554",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 1\n",
    "\n",
    "We define a **task** to be an *n-way k-shot* image classification problem. The maximum number of tasks that can be created from a dataset can be defined as $M \\choose N$$\\cdot$$ K \\choose k$ where $M$=the number of classes in the dataset, $N$=the number of classes in a task, $K$=the total number of images in a class, and $k$=the number of shots in a class. In this problem, we define a method to create an upper bound to the number of tasks used during training. \n",
    "\n",
    "**Problem**: In the `dataloader.py` file, complete the implementation of the `DataLoader.generate_task_list` method and the `DataLoader.data_generator` method. Details concerning the implementation are as follows:\n",
    "\n",
    "- The `generate_task_list` method generates a **list of dictionaries** where each dictionary comprises a **task** in the form of `{class_name: [random sequence of image indexes]}`\n",
    "    - For example, a 2-way 5-shot task for both the support & query datasets can be generated as `{'A': [1,3,5,7,9,2,4,6,8, 10], 'B': [8,6,4,10,2,1,5,3,7,9]}`\n",
    "    - Note that the length of the random sequence of image indexes should equal the # of shots in the support dataset $+$ # of shots in the query dataset.\n",
    "    \n",
    "- The `data_generator` method generates a **support** and a **query dataset** where each dataset is in the form of a numpy array with shape `[num_way, num_shot, image_width, image_height, num_channels]`.\n",
    "    - Note that `data_generator` is created based on tasks which have the form `{class_name: [random sequence of image indexes]}`\n",
    "    \n",
    "If the `dataloader.py` file is implemented correctly, the following code should run without error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "af2382c2-8f38-4179-a7bb-1387c551bb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing train Omniglot dataset\n",
      "Finished preprocessing\n",
      "Preprocessing test Omniglot dataset\n",
      "Finished preprocessing\n"
     ]
    }
   ],
   "source": [
    "from dataloader import DataLoader\n",
    "\n",
    "num_ways = 5\n",
    "support_shots = 5\n",
    "query_shots = 5\n",
    "\n",
    "train_dataset = DataLoader('train', n_way=num_ways, n_support=support_shots, n_query=query_shots)\n",
    "val_dataset = DataLoader('test', n_way=num_ways, n_support=support_shots, n_query=query_shots)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "441bff77-39a0-4950-8305-a964d802b361",
   "metadata": {},
   "source": [
    "**If implemented correctly, we can also visualize a random task using the `visualize_random_task` method.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e14ae5c-8949-4afb-90d7-99505264df52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAA+CAYAAABtAQ2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACdklEQVR4nO3dUU7iUBgF4L+TWYaJdR+6Cxfg1nx3T2LQFfhMOg+TMiijJVDbQ/m+hBcgcq9NDqflXmi6riuAFL/mHgDALqEERBFKQBShBEQRSkAUoQRE+T3w+BLWCzQHPMc8z8fQPC9hjlULnqemBEQRSkAUoQREEUpAFKEERBFKQBShBEQRSkCUocWT7Li7u6uqqvV6/eH+1Wo1w2hgmTQlIMqPNqWm+Xq1/Dl+42XfkF5eXj7c3zTNWc4HEmlKQJRm4B1+9Lf/vj1N2Cx+bHPj4+NjVVU9PDwkNKWL3sS546Q5ftfu27at5+fnU/78wcM44DmLPZaaEhBltlC6ubmZ66X5RtM029sl6rpu77YEu8e1v202m9psNnMPbY8lAXzQdd02kGY41eaHfF620rZtvb6+VlXV9fX1DCP6mtM3IIqmxJ7+nfPz0gfOV1ob+o6mBETRlNjTX3+41IvdzEtTAqIIJSCKUAKiCCUgir1vf528XypggeHo8wxdPDn5j1H2/4cl7X1rmmb7gcaMywXsfQPyWRIwklM/Pg9rI/zH+/v73EO4CJoSEGW2pjTULALOdw/WdV3d3t5WVdXb29vMo2FsT09PVVV1f39/VCNer9d1dXU19rAWa/IL3b2hfVUjhtFFf2HWJ5Ne6G7btqpG/2GFyS90b1/4yFP0I0LJhW6AFLM1pQlpSv8c1ZRONfJF/Nma0oQmaUrHmOJYakpAFEsC+JJlCsuVfGw1JSCKUAKiCCUgilACogglIIpQAqIIJSCKUAKiCCUgilACogglIIpQAqIIJSDK0PcpAUxKUwKiCCUgilACogglIIpQAqIIJSDKH1I3r1mYGAZ9AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAA+CAYAAABtAQ2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAACtklEQVR4nO3dMU7rWhQF0O2vX9LRIsFAYBYMIHTMhYIhAAOgYAx4GhF4AHTpeYUV/vsIkQhs58RZqyTRdY4Sto+vr+3m/f09AFX8s+sPAPA3oQSUIpSAUoQSUIpQAkoRSkAp/254fQ7rBZot3qPO/bGpzkOoMZlxnToloBShBJQilIBShBJQilACStl09g2+dHV1lSS5v79Pkriwm6HolIBSmg17uDns/nay5qNp+s1O2EFMWue6vv8NPk2t1in1Zvubdfg2kkM7nDm0eueoynfo8A0oRafEj1TZqzI/OiWglMlCqWmaLydHAf6mUwJKmWxOyRwEsA2dErC1rutGn4oRSkAplgQwmrZtkyQXFxcff3MYv9+Oj48/rncci04JKMW1b71J6hz5yvoyda59Ne/w/PycJDk/P//xsBteH/z6vsVikSS5u7sbauiNm97iPbP939QpAaXolHrqHMHj42OS5PLy8r8P8Pvu0F0CerOtUyj11Lk/hFJvtnU6fANKEUrwC23bpm3bj5MY/J5QAkopGUoPDw9ZrVZZrVa7/ijwreVymeVyOfqCwint+m4eJUMJOFwlz741TZPX19ckyenp6a+H2+I9sz2T8clgdb69vSVJrq+vkyS3t7c5OTkZavjvOPvWm22dJUNpYAf9BX8y2tNMzs7O8vLyMtTw3256w+u+y/1hSQBQn06pN1idO3xy7KR1rq/8X98JIPHctwHplACq0Cn11PlD607p6ekpNzc3Qw//lck7pSpPjv1ktr9ZnRJQik6pp879YU6pN9s6dUpAKUIJKGXUUGqaJl3Xpeu6MTcDzIhOCShl1EcsLRaLHB0djbkJYGZ0SkAplgT01Lk/LAnozbZOnRJQilACShFKQClCCShl00Q3wKR0SkApQgkoRSgBpQgloBShBJQilIBS/gB8JNIImAn70AAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAASUAAAA+CAYAAABtAQ2bAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAFxElEQVR4nO3dy07qUBQG4L9HhjjCoURMfACd61MoY2Hg3ER4CvUNSJSx+BT6GBol4ExG6kgJZ8BZtfSUXvduV+H/kpOccGm73WV1dd/qzGYzEBFp8afoAyAi8mJQIiJVGJSISBUGJSJShUGJiFRhUCIiVSoR76/CeAEnxmdYzvKIKuc6lBFY4XIyUyIiVRiUiEgVBiUiUoVBiYhUYVAiIlUYlCiTfr+Pfr9f9GGQZVLPjhOnYzAbBiUiUiVqnBLFMBwO/3utWq0CAGq1Wt6HY9X29jYA4O3tbeH1VquFMi+Dc319DQDodDrua6+vrwCAnZ2dVNuU8yLt99cVg1IGcVLZMv9QRaPRALAYfP0/WMdx/vsRyt9H49+gUpmf+tPpdOlnpNwSqC4vLxPtQ74PAKPRCMBvUKflePtGRKo4EVexzJc4uY05ODgAADw8PGTdZFLGhuxLWb6+vgAAh4eHAILLFJYleDMsg1mEsXJ2u10AwNXV1cLrtVoN7+/vwTsPKNPZ2RkAoNfr5VnOpTsKymzDjksa8FutVuRnw6Sob3XTTKQMjUYDLy8vxjYb9CIzJSJSxWqmlPTKZImRq07Sq12c9hTHcdRkSv6sAAjPBP2Gw6HbhuIvk+M4btZ1cXER4zBDpc6U2u02AODm5ibZDv/VZVimGOf7QHGZ0uPjIwDg6Oho+QYjzlWAmRIRrSErmZK3e1V6YqRnJmh/8p6lbvRMV5207T9ly5TkeCuVCr6/vxPvPCxTCnsvhdyXLvFmkWmOv6hMaTqdur2MQn5j+/v7bgYc91wF8smUrAQlKcBoNHJvAcKCknxebh2SptdRhxPjM5E/1qQnY9mCUlZRgcfg8IDC1lNKWl9bW1sAgMlkknTMU6q6DBui8vHxAeA3KAV9L05QGo1GJoc18PaNiPQzOnjSH6njRlTJkG5vbwEYz5Qy0Tjwj8pBmjFOT09z2Z+Nc1V+03LHk8fgT2ZKRKSKkTYlf4bk3aa8J1lQ2FXD0rSEQtpapCyDwQDHx8dLP8M2pcRK06aUZVcxPmPuxCluOhDblIhIPyNBaTabLfzLand318BR6eCddb6u8liDh1ZHbqsE5NXYVybr0ohewHxHKjHevhGRKlYzpaDFz9bF09MTAGBvb6/gI0lmMpm4t5ymhmZId3KQbrebeJ2iIjWbTQDA3d2dke212218fn4a3WbZMVMiIlWMDAmQuUH+dqOkc56kQbTT6Zi8ehbaVV62buS089TSfs/CvDCrdenuJGOdeteaCphexSEBRERaGB08aXl2eFrMlH5ZO5DxeIx6vQ4gfPKnyDDz3mimJOeurJMUtkKF93yOU8Ygk8kEwO9k3SVlN16XcVZCjVsPSQZER20q6EUjDd2mf3Tj8dj9v+aF1i0t51BK3nra3NwEoHvIg3/sVESQALA4w1/KGHe5HX+nz8nJSfyDVUoW7DM93Ie3b0SkispHLNXrdZPLp1pjOxOQBfzL0mXuX9AvjDTueh9DlBfp1k9D6lyWl5Xb0F6vF/o9OZ8lI8t7MLHJc1W2ZWukPjMlIlLF+iOWlkVTaSTzkqtntVp1GxINKLyhO22jaMIGSDWP5QlrCJWF6yXTSHEFN9LQnfYhAjnJtS5lgO/z8/N8wwkbvDNkYRwSQET6Wc+U/LzrFvulfTxyhMIzJXcndgcjqsmUlj3McuFADF9dvZtOu2FFcq1Lf31F1Y3/Me6m6zL3oFSAUgYlDc8KU4pBac7awoQbGxv4+flZeO/+/h4AcH5+7g7ZMdAZxds3ItJP5ZCAdWZyfhVREt6VLcK6+20P12GmRESqsE1pzlo5m80mBoNB4u9Z6CoH1qM+16GMwAqXk5kSEanCTGkul3LGeSBChom9asppGTOluZUtJ4PSHMtZHgxKcytbTt6+EZEqDEpEpAqDEhGpEtWmRESUK2ZKRKQKgxIRqcKgRESqMCgRkSoMSkSkCoMSEanyFwVYur7EJNfDAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "train_dataset.visualize_random_task()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ac3d864-c566-4f3f-b607-f10668d246df",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Problem 2\n",
    "\n",
    "Next we will implement the prototypical network and examine how the performance of the model changes when **the total number of tasks is changed from 1k to 10k**.\n",
    "\n",
    "As discussed in the lecture, the basic idea of protonets is to learn a mapping $f_\\theta(\\cdot)$ from images to features such that images of the same class are close to each other in a feature space. Central to this is the notion of a *prototype*\n",
    "\n",
    "$$c_n = \\frac{1}{K} \\sum_{(x,y)\\in\\mathcal{D}^{tr}_i:y=n} f_\\theta(x)$$\n",
    "\n",
    "i.e. for task $i$, the prototype of the $n$-th class $c_n$ is defined as the mean of the $K$ feature\n",
    "vectors of that class’s support images. To classify some image $x$, we compute a measure of\n",
    "distance $d$ between $f_\\theta(x)$ and each of the prototypes. We will use the squared Euclidean\n",
    "distance:\n",
    "\n",
    "$$d(f_\\theta(x),c_n) = ||f_\\theta(x) - c_n ||^2_2$$\n",
    "\n",
    "We interpret the negative squared distances as **logits**, or *unnormalized log-probabilities,\n",
    "of* $x$ *belonging to each class*. To obtain the proper probabilities, we apply the softmax\n",
    "operation:\n",
    "\n",
    "$$p_\\theta(y = n|x) = \\frac{\\exp(-d(f_\\theta(x),c_n))}{\\sum^N_{n'=1}\\exp(-d(f_\\theta(x),c_{n'}))}$$\n",
    "\n",
    "Because the softmax operation preserves ordering, the class whose prototype is closest to\n",
    "$f_\\theta(x)$ is naturally interpreted as the most likely class for $x$. To train the model to generalize,\n",
    "we compute prototypes using support data, but minimize the negative log likelihood of the query data\n",
    "\n",
    "$$\\mathcal{J}(\\theta) = \\mathbb{E}_{\\mathcal{T}\\sim p(\\mathcal{T}), (\\mathcal{D}^{tr}, \\mathcal{D}^{ts})\\sim \\mathcal{T}} \\left[ \\frac{1}{NQ} \\sum_{(x^{ts},y^{ts})\\sim\\mathcal{D}^{ts}} -\\log p_\\theta(y=y^{ts} | x^{ts})\\right]$$\n",
    "\n",
    "Notice that this is equivalent to using a cross-entropy loss.\n",
    "\n",
    "We optimize $\\theta$ using Adam, and as is standard for stochastic gradient methods, we approximate the objective for $\\mathcal{J}$ with\n",
    "Monte Carlo estimation on minibatches of tasks. Thus, for one minibatch of size $B$, we have\n",
    "\n",
    "$$\\mathcal{J}(\\theta) \\approx \\frac{1}{B}\\sum_{i=1}^B \\left[ \\frac{1}{NQ} \\sum_{(x^{ts},y^{ts})\\sim\\mathcal{D}^{ts}} -\\log p_\\theta(y=y^{ts} | x^{ts})\\right]$$\n",
    "\n",
    "\n",
    "**Problem:**\n",
    "\n",
    "1. In the `model.py` file, complete the implementation of the `Prototypical_Network.call` method. Pay attention to the inline comments.\n",
    "2. Test the model with different amounts of training tasks (e.g. from 1k to 10k).\n",
    "    1.  Create a plot of the validation accuracy using `matplotlib` and report your findings. \n",
    "        - Train and validate the model on 5-way 5-shot tasks for both the support and query datasets.\n",
    "        - [Hint]: you should obtain a query accuracy on the validation split of **at least** 97% with 10k tasks.\n",
    "    2. Also plot how the degree of overfitting (the difference between training accuracy and validation accuracy) changes with different amounts of tasks.\n",
    "3. Repeat (2) with different tasks [5way-1shot, 20way-5shot, 20way-1shot] and report your findings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bcf817cd-de2e-463c-9016-64cee359d2b9",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch1\n",
      "(25, 28, 28, 1)\n",
      "(25, 28, 28, 1)\n",
      "(50, 28, 28, 1)\n",
      "(50, 64)\n",
      "(25, 64)\n",
      "(5, 5, 64)\n",
      "(5, 64)\n",
      "(5, 25)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "in user code:\n\n    File \"/home/jshyeon/metalearning_hw/Assignment1/train.py\", line 16, in train  *\n        gradients = tape.gradient(loss, model.trainable_variables)\n\n    TypeError: Cannot convert the argument `type_value`: 0.0 to a TensorFlow DType.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-4ba283dcbdbf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      8\u001b[0m metrics = train.train_model(train_dataset, val_dataset, \n\u001b[1;32m      9\u001b[0m                             \u001b[0mn_tasks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_tasks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m                             n_tpe=num_tasks_per_epoch)\n\u001b[0m",
      "\u001b[0;32m~/metalearning_hw/Assignment1/train.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_dataset, val_dataset, n_tasks, n_epochs, n_tpe, is_random)\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0mval_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_query\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m             \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msupport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0mvalidate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_support\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_query\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mon_end_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_accs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_losses\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_accs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 153\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    154\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1146\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1147\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1148\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1149\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: in user code:\n\n    File \"/home/jshyeon/metalearning_hw/Assignment1/train.py\", line 16, in train  *\n        gradients = tape.gradient(loss, model.trainable_variables)\n\n    TypeError: Cannot convert the argument `type_value`: 0.0 to a TensorFlow DType.\n"
     ]
    }
   ],
   "source": [
    "import train\n",
    "\n",
    "num_tasks = 10000 # The total number of tasks in the predefined task distribution\n",
    "num_epochs = 100 # The number of epochs to train the model\n",
    "num_tasks_per_epoch = 100 # The number of tasks to be trained on for each epoch\n",
    "\n",
    "# metrics = [train accuracies, train losses, validation accuracies, validation losses]\n",
    "metrics = train.train_model(train_dataset, val_dataset, \n",
    "                            n_tasks=num_tasks, n_epochs=num_epochs, \n",
    "                            n_tpe=num_tasks_per_epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a24e082",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(\n",
      "[[-1  1  1]\n",
      " [ 2  4  4]\n",
      " [ 0  0  2]\n",
      " [ 3  3  5]\n",
      " [ 0  0  2]\n",
      " [ 3  3  5]\n",
      " [-1  0  2]\n",
      " [ 2  3  5]], shape=(8, 3), dtype=int32)\n",
      "tf.Tensor([ 3 36  4 43  4 43  5 38], shape=(8,), dtype=int32)\n",
      "tf.Tensor(176, shape=(), dtype=int32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 3, 36,  4, 43,  4, 43,  5, 38])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "tensor1 = tf.constant([[1,2,3],[4,5,6]])\n",
    "t = tf.constant([2,1])\n",
    "\n",
    "\n",
    "tensor1= tf.tile(tensor1, [4,1])\n",
    "\n",
    "tensor2 = tf.constant([[2,1,2],[1,2,1],[1,2,1],[2,2,1]])\n",
    "\n",
    "tensor2 = tf.repeat(tensor2,2, axis=0)\n",
    "\n",
    "t= tensor1-tensor2\n",
    "print(t)\n",
    "a=tf.math.reduce_sum(tf.square(t),axis=1)\n",
    "b=tf.math.reduce_sum(tf.square(t))\n",
    "print(a)\n",
    "print(b)\n",
    "np.sum(np.square(t),axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b1a6395-a092-440c-92be-d0bf79a0db74",
   "metadata": {},
   "source": [
    "## Problem 3\n",
    "\n",
    "Finally, we will evaluate the generalization performance of the model by using a **different test setting from a training setting**.\n",
    "\n",
    "<br>\n",
    "\n",
    "As prototypical networks are based on a nonparametric method, we can flexibly train the model with different types of tasks (e.g. 5way1shot tasks + 5way5shot tasks). Let's assume we are allowed to use up to 30 training samples to train a single task. Evaluate the performance of the model when we train the model using **randomly defined training tasks**.\n",
    "\n",
    "\n",
    "\n",
    "**Problem**:\n",
    "\n",
    "1. In the `dataloader.py` file, implement the `random_data_generator` method.\n",
    "2. Using the experimental details outlined below, run experiments and compare with the results from Problem 2\n",
    "    - Note that you do not need to specify a set number of tasks for this problem\n",
    "\n",
    "\n",
    "**Experimental Details**:\n",
    "- *Train setting*: Assume to use exactly 30 images for each task (for support + query). First randomly select $N$ from the set $\\{2, 3, 5, 6, 10, 15\\}$. This means that each $N$ can use $\\{15, 10, 6, 5, 3, 2\\}$ samples per class respectively. By properly splitting the number of samples into 2 splits (support/query), we can randomly define different tasks.\n",
    "    - For example, an example support/query task can be defined as support={5-way 4-shot}, and query={5-way 2-shot}.\n",
    "    - *Extra Credit (optional)*: implement and test a method where we use a flexible number of images (not limited to just 30) where $N$ can also be defined randomly.\n",
    "- *Test setting*: 5way 5 shot (5$\\cdot$5 for support + 5$\\cdot$1 for query = 30 images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b81f55-0b23-41b0-9a2c-44164eb58f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ways = 5\n",
    "support_shots = 5\n",
    "query_shots = 1\n",
    "\n",
    "train_dataset = DataLoader('train')\n",
    "val_dataset = DataLoader('test', n_way=num_ways, n_support=support_shots, n_query=query_shots)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d925446-05fd-4ea0-9422-aab4c82e0890",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10 # The number of epochs to train the model\n",
    "num_tasks_per_epoch = 100 # The number of tasks to be trained on for each epoch\n",
    "\n",
    "# metrics = [train accuracies, train losses, validation accuracies, validation losses]\n",
    "metrics = train.train_model(train_dataset, val_dataset, \n",
    "                            n_tasks=num_tasks, n_epochs=num_epochs, \n",
    "                            n_tpe=num_tasks_per_epoch, is_random=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
